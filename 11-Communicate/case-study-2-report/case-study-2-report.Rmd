---
title: "Examining the Effect of Gender on Graduate Program Admission"
author: "Jake Thompson"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
bibliography: ["bib/references.bib", "bib/packages.bib", "bib/knit.bib"]
biblio-style: apalike2
csl: csl/apa.csl
link-citations: true
output:
  bookdown::html_document2:
    theme: cosmo
    number_sections: false
---

```{r setup, include = FALSE}
library(tidyverse)
library(broom)
library(rsample)
library(tidydscompanion)
library(knitr)
library(hrbrthemes)
library(colorblindr)

knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  error = FALSE,
  fig.path = "figures/",
  fig.retina = 3,
  fig.width = 8,
  fig.asp = 0.618,
  fig.align = "center",
  out.width = "90%"
)
```

Admissions into graduate school is important for many reasons. In most academic programs, admission requires an application that includes test scores and undergraduate grade point average (GPA). However, there are other factors of an applications that should not be considered when deciding whether or not to admit an individual into a graduate program. For example, an individual's gender should not impact the admission decision. Stated another way, given comparable academic credentials, individuals of different genders should have the same probability of acceptance. In this report, we explore the effect of applicants' gender on the admission decision for a made-up university. Using one of the more popular graduate admissions tests, the Graduate Record Examination [GRE; @gre].

## Data

```{r read-data}
data("admission", package = "tidydscompanion")

# calculate summary statistics
total <- nrow(admission)
total_admit <- sum(admission$admit)
rate <- round((total_admit / total) * 100, digits = 0)
total <- prettyNum(total, big.mark = ",")
total_admit <- prettyNum(total_admit, big.mark = ",")

gender_counts <- admission %>%
  count(gender) %>%
  pivot_wider(names_from = gender, values_from = n)

male_total <- gender_counts$Male
female_total <- gender_counts$Female
male_admit <- sum(admission$admit[admission$gender == "Male"])
female_admit <- sum(admission$admit[admission$gender == "Female"])
male_rate <- round((male_admit / male_total) * 100, digits = 0)
female_rate <- round((female_admit / female_total) * 100, digits = 0)

male_total <- prettyNum(male_total, big.mark = ",")
female_total <- prettyNum(female_total, big.mark = ",")
```

Data was collected for `r total` make-believe students. In total, there were `r male_total` males and `r female_total` females included in the sample. Of the `r total` applicants, `r total_admit` (`r rate`%) were admitted into the graduate program, including `r male_rate`% of male appicants and `r female_rate`% of female applicants. The GRE consists of three parts:

1. Verbal Reasoning
2. Quantitative Reasoning
3. Analytical Writing

The verbal and quantitative reasoning subtests are scored between 130 and 170 in integer increments, with an average score of 150. The writing subtest is scored on a 0 to 6 scale in 0.5 increments. Table \@ref(tab:gender-summary) shows the median scores for males and females on each section of the GRE, as well as the median GPA. This indicates that on average male and female applicants had similar scores on academic measures.

```{r gender-summary}
admission %>%
  group_by(gender) %>%
  summarize(n = n(),
            gre_v = mean(gre_v),
            gre_q = mean(gre_q),
            gre_w = mean(gre_w),
            gpa = mean(gpa)) %>%
  kable(align = "c", booktabs = TRUE, digits = 2,
        format.args = list(big.mark = ","),
        caption = "Median test scores and GPA, by Gender",
        col.names = c("Gender", "n", "Verbal", "Quantitative", "Writing",
                      "GPA"))
```

## Method

```{r rsample-params, cache = TRUE}
boot_samples <- 100
folds <- 10
repeats <- 10
```

```{r single-mod}
# set seed for reproducibility
set.seed(32011)

# run a single model on the full data set
single_mod <- glm(admit ~ gre_v * gre_q + gre_w + gpa + gender,
                  data = admission, family = "binomial") %>%
  augment(type.predict = "response") %>%
  rowid_to_column(var = "id")

# sample example applicants for the text
exm1 <- single_mod %>%
  filter(admit == 1, between(.fitted, 0.51, 0.55)) %>%
  sample_n(1)
exm2 <- single_mod %>%
  filter(admit == 1, between(.fitted, 0.95, 0.99)) %>%
  sample_n(1)
exm3 <- single_mod %>%
  filter(admit == 1, between(.fitted, 0.45, 0.49)) %>%
  sample_n(1)
```

In order to evaluate the effect of gender on admissions decisions, two types of analyses were conducted. First, the difference in average academic indicators between males and females was examined using bootstrapping. Second, v-fold cross validation was using to compare the predictive ability of logistic regression models that include gender as a predictor versus models that do not.

All analyses were conducted in R version `r getRversion()` [@R-base]. In the first analysis, `r prettyNum(boot_samples, big.mark = ",")` bootstrap samples were generated using the rsample package [@R-rsample]. Then, for each bootstrap sample, the difference between the average score of males and females was calculated for each of the academic indicators: GRE Verbal Reasoning, GRE Quantitative Reasoning, GRE Analytical Writing, and GPA. If academic performance is consistent across groups (as indicated in Table \@ref(tab:gender-summary)), the distributions of each difference should be centered around zero. A *t*-test was also conducted on each measure of academic performance for the purpose of comparison to the bootstrap results.

In the second analysis, a `r prettyNum(folds, big.mark = ",")`-fold cross validation with `r prettyNum(repeats, big.mark = ",")` repeat was conducted using the rsample package [@R-rsample]. For each fold, three models were estimated on the analysis (also called 'training') data.

* __Empty.__ In this model, the probability of admission for each applicant is equal to the overall admission rate.
* __Academics.__ In this model, the probability of admission for each applicant is predicted by only the GRE scores and GPA.
* __Full.__ In this model, the probability  of admission for each applicant is predicted by all academic indicators, as well as applicant gender.

Model performance was assessed using the assessment (also known as 'test') data, which was not included in the model estimation. The predictive ability of model was evaluated using the proportion of correctly classified applicants and the Log Loss for the fitted probabilities for each applicant in the assessment data. The correct classification rate measures whether the predicted outcome matches the observed outcome. Because the predicted outcome is on a probability scale, applicants with predicted probabilities greater than 0.50 are predicted to be admitted, and those with a predicted probability less than 0.50 are predicted to not be admitted.

The correct classification rate provides a high level overview of model performance, but is not always the best or most effective indicator. Take for example Applicant A, who was admitted and had a `r sprintf("%0.2f", pull(exm1, .fitted))` probability of being accepted (when using the full model on the entire data set). Similarly, Applicant B was also accepted but had a `r sprintf("%0.2f", pull(exm2, .fitted))` probability of being admitted. Clearly, the `r sprintf("%0.2f", pull(exm2, .fitted))` probability for Applicant B is a better prediction, even though both are counted the same for the correct classification rate. Conversely, Applicant C had a `r sprintf("%0.2f", pull(exm3, .fitted))` probability of being accepted but was admitted. Using correct classification rate, this prediction would be incorrect, even though it is very similar to the `r sprintf("%0.2f", pull(exm1, .fitted))` probability of Applicant A, which was considered correct.

Instead of dichotomizing the predicted probabilities at 0.5, a better measure would evaluate the actual values of the probabilities in relation to the observed outcome. One such measure is the Log Loss. This measure penalizes predictions based on how far the fitted probabilities are from the observed outcome, with lower scores indicating more accurate predictions. The Log Loss is calculated as shown in equation \@ref(eq:log-loss), where $n$ is the total sample size of the assessment data, $\hat{y_i}$ is the model predicted probability of admission, and $y_i$ is the observed admission decision.

\begin{equation}
  \text{LogLoss} = -\frac{1}{n}\sum_{i=1}^n[y_i\log(\hat{y_i}) + (1-y_i)\log(1-\hat{y_i})]
  (\#eq:log-loss)
\end{equation}

Because the $\log$ of the fitted probabilities is used, this measure heavily penalizes models that are confident in incorrect classifications. Figure \@ref(fig:logloss-exm) shows the penalty for a single observation with an observed outcome of 1. There is a gradual slope away from 1, but as the predicted probability gets closer to 0 (i.e., more confident in a prediction of 0), the penalty increases rapidly. As demonstrated in equation \@ref(eq:log-loss), the overall Log Loss is the average of the penalties for each observation. Thus, having confidently incorrect predictions can have a large impact on the overall Log Loss score.

```{r logloss-exm, fig.cap = "Penalty for a single observation with an observed outcome of 1."}
tibble(x = seq(0.0001, 1, 0.0001)) %>%
  mutate(logloss = -1 * log(x)) %>%
  ggplot(aes(x = x, y = logloss)) +
    geom_line() +
    labs(x = expression(hat(y)[i]),
         y = "Log Loss") +
    theme_ipsum()
```

## Results

### Comparison of Academic Indicators

```{r calc-bootstrap, cache = TRUE, dependson = "rsample-params"}
# define function to calculate difference on all academic indicators between
# males and females
mean_diff <- function(splits) {
  x <- analysis(splits)
  
  x %>%
    select(gender, gre_v, gre_q, gre_w, gpa) %>%
    group_by(gender) %>%
    summarize_all(mean) %>%
    pivot_longer(-gender, names_to = "indicator", values_to = "score") %>%
    pivot_wider(names_from = gender, values_from = score) %>%
    mutate(diff = Male - Female) %>%
    select(indicator, diff) %>%
    pivot_wider(names_from = indicator, values_from = diff)
}

# set random seed for reproducibility
set.seed(32011)

# create bootstrap samples
bootstrap_results <- admission %>%
  bootstraps(times = boot_samples) %>%
  mutate(results = map(splits, mean_diff)) %>%
  unnest(results) %>%
  pivot_longer(gre_v:gpa, names_to = "indicator", values_to = "score") %>%
  mutate(indicator = factor(indicator,
                            levels = c("gre_v", "gre_q", "gre_w", "gpa"),
                            labels = c("Verbal Reasoning",
                                       "Quantitative Reasoning",
                                       "Analytical Writing", "GPA")))
```

```{r t-tests}
t_tests <- admission %>%
  rowid_to_column(var = "applicant") %>%
  select(-admit) %>%
  pivot_longer(gre_v:gpa, names_to = "indicator", values_to = "score") %>%
  group_by(indicator) %>%
  nest() %>%
  mutate(t_test = map(data, ~ t.test(score ~ gender, data = .x)),
         t = map_dbl(t_test, "statistic"),
         df = map_dbl(t_test, "parameter"),
         pval = map_dbl(t_test, "p.value"),
         indicator = factor(indicator,
                            levels = c("gre_v", "gre_q", "gre_w", "gpa"),
                            labels = c("Verbal Reasoning",
                                       "Quantitative Reasoning",
                                       "Analytical Writing", "GPA"))) %>%
  select(Measure = indicator, t, df, `p-value` = pval)
```

The distributions of the difference in average scores between males and females can be see in Figure \@ref(fig:diff-plot). As expected given the summary statistics in Table \@ref(tab:gender-summary), all differences are centered around zero. There is more variability in the differences for the GRE Verbal Reasoning and GRE Quantitative Reasoning scores; however, this is also expected given larger variability and range of possible scores for these measures. Table \@ref(tab:bootstrap-sum) shows the empirical 95% confidence interval for each difference, calculated as the 2.5 and 97.5 percentiles of the distribution for each difference.

(ref:diff-plot) Average performance on academic indicators for males compared to females over `r prettyNum(boot_samples, big.mark = ",")` bootstrapped samples.

```{r diff-plot, fig.cap = "(ref:diff-plot)"}
ggplot(bootstrap_results, aes(x = score, color = indicator)) +
  geom_density(aes(fill = indicator, color = indicator), alpha = 0.8,
               show.legend = FALSE) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  facet_wrap(~ indicator, scales = "free_y", nrow = 2) +
  expand_limits(x = c(-1, 1)) +
  scale_color_OkabeIto() +
  scale_fill_OkabeIto() +
  labs(x = "Average score for males compared to females", y = "Density") +
  theme_ipsum()
```

```{r bootstrap-sum}
bootstrap_results %>%
  group_by(Measure = indicator) %>%
  summarize(Mean = mean(score),
            Median = median(score),
            `2.5%` = quantile(score, probs = 0.025),
            `97.5%` = quantile(score, probs = 0.975)) %>%
  kable(align = c("l", rep("r", 4)), booktabs = TRUE, digits = 2,
        caption = "Summary of bootstrapped distributions of the difference in academic indicators between males and females")
```

Finally, a *t*-test was conducted for each measure of academic performance. Equal variance was not assumed across group. Thus, the Welch (or Satterthwaite) approximation for degrees of freedom was used. The results of all the tests can be seen in Table \@ref(tab:bootstrap-t). All *t*-tests were non-significant at an alpha level of .05. This is consistent with the bootstrapped confidence intervals (Table \@ref(tab:bootstrap-sum)) and overall distributions (Figure \@ref(fig:diff-plot)), which also indicated no differences between the two groups on any of the academic performance measures.

(ref:bootstrap-t) *t*-tests for difference in mean performance on each academic measure between males and females

```{r bootstrap-t}
t_tests %>%
  mutate(df = round(df, digits = 0)) %>%
  kable(align = c("l", "r", "r", "r"), booktabs = TRUE, digits = 2,
        format.args = list(big.mark = ","),
        caption = "(ref:bootstrap-t)")
```

### Modeling the Effect of Gender

```{r calc-vfold, cache = TRUE, dependson = "rsample-params"}
# define the formulas for each model
empty <- as.formula(admit ~ 1)
academics <- as.formula(admit ~ gre_v * gre_q + gre_w + gpa)
full <- as.formula(admit ~ gre_v * gre_q + gre_w + gpa + gender)

# define function for fitting model and applying to assessment data
holdout_results <- function(splits, ...) {
  # Fit the model to the 90%
  mod <- glm(..., data = analysis(splits), family = binomial)
  
  # Save the 10%
  holdout <- assessment(splits)
  
  # `augment` will save the predictions with the holdout data set
  res <- broom::augment(mod, newdata = holdout, type.predict = "response") %>%
    mutate(prediction = ifelse(.fitted > 0.5, 1L, 0L),
           correct = prediction == admit)
  
  # Return the assessment data set with the additional columns
  res
}

# set random seed for reproducibility
set.seed(32011)

# create v-fold samples
vfold_results <- admission %>%
  vfold_cv(v = folds, repeats = repeats) %>%
  mutate(empty_mod = map(splits, holdout_results, empty),
         acadm_mod = map(splits, holdout_results, academics),
         compl_mod = map(splits, holdout_results, full)) %>%
  ungroup() %>%
  select(-splits) %>%
  pivot_longer(contains("mod"), names_to = "model", values_to = "results") %>%
  unnest(results) %>%
  group_by(id, id2, model) %>%
  summarize(pct_cor = mean(correct),
            logloss = -1 * mean((admit * log(.fitted)) + ((1 - admit) * log(1 - .fitted))))
```

The `r prettyNum(folds, big.mark = ",")`-fold cross validation procedure was repeated `r prettyNum(repeats, big.mark = ",")` times for a total of `r prettyNum(folds * repeats, big.mark = ",")` resamples that were used to estimate and then evaluate the predictive accuracy of each of the three models. The first measure of predictive accuracy of the models is the correct classification rate. For this analysis, an applicant was considered correctly classified if their model-predicted probability of admission was greater than .5 and they were admitted, or their model-predicted probability was less than .5 and they were not admitted. Figure \@ref(fig:ccr-dist) shows this distribution of correct classification rates across all resamples for each of the investigated models. The academics only and full models both have distinctly higher correct classification rates than the empty model. The full model has a slightly higher correct classification rate the than academics only model on average, but there is a great deal of overlap between the two distributions.

```{r ccr-dist, fig.cap = "Distributions of applicant correct classification rate."}
ggplot(vfold_results, aes(x = pct_cor)) +
  geom_density(aes(color = model, fill = model), alpha = 0.8) +
  scale_color_OkabeIto(limits = c("compl_mod", "acadm_mod", "empty_mod"),
                       breaks = c("empty_mod", "acadm_mod", "compl_mod"),
                       labels = c("Empty", "Academics", "Full")) +
  scale_fill_OkabeIto(limits = c("compl_mod", "acadm_mod", "empty_mod"),
                      breaks = c("empty_mod", "acadm_mod", "compl_mod"),
                      labels = c("Empty", "Academics", "Full")) +
  expand_limits(x = c(0.6, 0.9)) +
  scale_x_percent() +
  labs(x = "Correct Classification Rate", y = "Density", color = "Model",
       fill = "Model") +
  theme_ipsum_ps() +
  theme(legend.position = "bottom") +
  guides(fill = guide_legend(override.aes = list(alpha = 1)))
```

The second measure of predictive accuracy to be evaluated was the Log Loss, as defined in equation \@ref(eq:log-loss). Figure \@ref(fig:logloss) shows the median Log Loss values for call models, along with 80%, 95%, and 99% confidence intervals calculated from the empirical distribution of values estimated across all `r prettyNum(folds * repeats, big.mark = ",")` cross validation resamples. As with the correct classification rate, the academics and full models both outperform the empty model. Once again the full model show the best performance, as it has the lowest median Log Loss. However, there is a also a great deal of overlap in the confidence intervals for the Log Loss of the academics only and full models. 

```{r logloss, fig.cap = "Log Loss of competing models."}
vfold_results %>%
  group_by(model) %>%
  summarize(med = median(logloss),
            lb_99 = quantile(logloss, prob = 0.005),
            ub_99 = quantile(logloss, prob = 0.995),
            lb_95 = quantile(logloss, prob = 0.025),
            ub_95 = quantile(logloss, prob = 0.975),
            lb_80 = quantile(logloss, prob = 0.100),
            ub_80 = quantile(logloss, prob = 0.900)) %>%
  pivot_longer(contains("_"), names_to = "boundary", values_to = "value") %>%
  separate(boundary, into = c("bound", "level")) %>%
  pivot_wider(names_from = bound, values_from = value) %>%
  mutate(level = paste0(level, "%")) %>%
  ggplot() +
  geom_errorbarh(
    aes(y = model, xmin = lb, xmax = ub, color = level, size = level),
    height = 0
  ) +
  geom_point(aes(x = med, y = model), color = "#E69F00", size = 3) +
  expand_limits(x = c(0.2, 0.7)) +
  scale_y_discrete(limits = c("compl_mod", "acadm_mod", "empty_mod"),
                   labels = c("Full", "Academics", "Empty")) +
  scale_color_manual(values = c(
    `80%` = darken("#56B4E9", .2),
    `95%` = "#56B4E9",
    `99%` = lighten("#56B4E9", .4)
  )) +
  scale_size_manual(values = c(`80%` = 4, `95%` = 3, `99%` = 2)) +
  labs(x = "Median Log Loss", y = NULL, size = "Confidence Level",
       color = "Confidence Level") +
  theme_ipsum_ps() +
  theme(legend.position = "bottom")
```

## Discussion

In this analysis, the effect of gender on admission into a graduate program was examined. The descriptive statistics show a difference in proportion of applicants admitted by gender, with `r male_rate`% of males and `r female_rate`% of females being accepted. Further, the analysis of the bootstrapped resampling showed no meaningful differences on any of the academic measures. Given this information, it would be expected that males and females would have the same rates of acceptance. However, when acceptance into the program is modeled parametrically, the addition of gender to the model did not significantly improve the predictive ability of the model over what was acheived with academic indicators alone. Thus, the evidence from the cross validation analyses does not support the theory that gender has a meaningful impact on an applicant's admission status.

Given the conflicting evidence, further investigation is warranted. Notably, this investigation did not inlcude many variables that factor into an admission decision including interviews, personal statements, and letters of reference. All of these variables play an important role in the review and admission process, and thus would be important to include in a more thorough analysis.

### Colophon

This report was written in R Markdown using the rmarkdown [@R-rmarkdown] and bookdown [@R-bookdown] packages. All graphics were created using the ggplot2 package [@R-ggplot2], with theming and coloring provided by the hrbrthemes [@R-hrbrthemes] and colorblindr [@R-colorblindr] packages respectively. Tables were created with the knitr package [@R-knitr]. Analyses were completed using the dplyr [@R-dplyr], purrr [@R-purrr], stats [@R-base], tibble [@R-tibble], and tidyr [@R-tidyr] packages.

## References

```{r write-packages, include = FALSE}
if (!file.exists("bib/packages.bib")) file.create("bib/packages.bib")
if (!file.exists("bib/knit.bib")) file.create("bib/knit.bib")
suppressWarnings(
  knitr::write_bib(c("rmarkdown", "bookdown"), "bib/knit.bib")
)
suppressWarnings(
  knitr::write_bib(c(.packages()), "bib/packages.bib")
)
```
